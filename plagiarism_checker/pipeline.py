"""
æŠ„è¢­æ£€æµ‹çš„ä¸»è¦æµç¨‹æ§åˆ¶ï¼šåŠ è½½è¯­æ–™ã€å‘é‡åŒ–ã€ç´¢å¼•æ„å»ºã€ç›¸ä¼¼é…å¯¹ã€èšåˆä¸æŠ¥å‘Šè¾“å‡ºã€‚
æ”¯æŒå¥å­çº§/æ®µè½çº§æ£€æµ‹ã€å¼•ç”¨æƒ©ç½šã€å¹¶è¡ŒåŠ é€Ÿã€è·¨è¯­è¨€ä¸æ™ºèƒ½Agentåˆ†æã€‚
"""

from __future__ import annotations

from dataclasses import dataclass
import json
from pathlib import Path
from typing import List, Tuple

import numpy as np

from .corpus import SentenceRecord, load_corpus, load_paragraphs
from .embedder import (
    build_embeddings,
    build_embeddings_parallel,
    build_multilingual_embeddings,
    build_index,
)
from .similarity import (
    detect_pairs,
    detect_paragraph_pairs,
    detect_pairs_crossset,
    detect_paragraph_pairs_crossset,
    aggregate_pairs,
    aggregate_paragraph_pairs,
    build_pair_details,
)
from .reporting import (
    write_summary_csv,
    write_pair_results,
    write_evidence_top,
    write_paragraph_summary,
    write_word_report,
    write_word_summary_report,
)


@dataclass
class PipelineConfig:
    submissions_dir: Path = Path("./paraphrase_outputs")
    model_name: str = "all-MiniLM-L6-v2"
    device: str | None = None              # è®¾å¤‡é€‰æ‹©ï¼šNoneè‡ªåŠ¨, 'cpu', 'cuda'
    use_parallel: bool = False             # CPUå¤šçº¿ç¨‹å¹¶è¡ŒåŠ é€Ÿ
    num_workers: int = 2                   # å¹¶è¡Œworkeræ•°é‡
    index_top_k: int = 5
    similarity_threshold: float = 0.82
    max_hits_per_pair: int = 50
    output_dir: Path = Path(".")
    
    # æ–°å¢åŠŸèƒ½å¼€å…³
    enable_paragraph_check: bool = True    # æ®µè½çº§æ£€æµ‹å¼€å…³
    enable_citation_check: bool = True     # å¼•ç”¨æƒ©ç½šå¼€å…³ï¼ˆæ–¹å‘æ€§ï¼‰
    enable_multilingual: bool = False      # è·¨è¯­è¨€æ£€æµ‹å¼€å…³
    
    # æ®µè½æ£€æµ‹å‚æ•°
    para_top_k: int = 3
    para_threshold: float = 0.75

    # æ–°å¢Agenté…ç½®
    enable_agent: bool = False             # æ™ºèƒ½Agentåˆ†æå¼€å…³
    agent_threshold: float = 0.70          # è§¦å‘åˆ†æçš„é£é™©åˆ†æ•°é˜ˆå€¼
    api_config_path: str = "api_config.json"  # Agent APIé…ç½®è·¯å¾„
    agent_max_reports: int = 3
    agent_dual_phase: bool = False

    # ç›®æ ‡æ¨¡å¼è¿‡æ»¤ï¼šä»…ä¿ç•™ (å·¦âˆˆtargets, å³âˆˆreferences) çš„æ–¹å‘æ€§å¯¹
    target_stems: List[str] | None = None
    reference_stems: List[str] | None = None


class PlagiarismPipeline:
    """ç«¯åˆ°ç«¯çš„æŠ„è¢­æ£€æµ‹æµç¨‹ã€‚å°†é…ç½®ä¸å„å¤„ç†é˜¶æ®µç»„ç»‡ä¸ºå¯å¤ç”¨ç®¡é“ã€‚"""

    def __init__(self, config: PipelineConfig) -> None:
        self.config = config

    def run(self) -> Tuple[List[dict], List[dict]]:
        """
        æ‰§è¡Œå¥å­çº§æ£€æµ‹æµç¨‹ã€‚

        Returns:
            (sent_stats, sent_details): å¥å­çº§ç»Ÿè®¡ä¸è¯¦ç»†å‘½ä¸­ã€‚
        """
        cfg = self.config
        
        # 1) åŠ è½½è¯­æ–™
        rows = load_corpus(cfg.submissions_dir)
        if not rows:
            raise RuntimeError(f"{cfg.submissions_dir} é‡Œæ²¡æ‰¾åˆ°æœ‰æ•ˆæ–‡æœ¬")

        # 2) é€‰æ‹©æ¨¡å‹å¹¶å‘é‡åŒ–ï¼ˆæ”¯æŒå¤šè¯­è¨€/å¹¶è¡Œ/å•æœºï¼‰
        if cfg.enable_multilingual:
            model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            embeddings = build_multilingual_embeddings(
                [row.text for row in rows],
                model_name=model_name,
                device=cfg.device,
            )
        elif cfg.use_parallel and (cfg.device is None or cfg.device == 'cpu'):
            embeddings = build_embeddings_parallel(
                [row.text for row in rows],
                model_name=cfg.model_name,
                device='cpu',
                num_workers=cfg.num_workers,
            )
        else:
            embeddings = build_embeddings(
                [row.text for row in rows],
                model_name=cfg.model_name,
                device=cfg.device,
            )

        # 3) å»ºç«‹ç›¸ä¼¼åº¦ç´¢å¼•
        index = build_index(embeddings)
        
        # 4) å¥å­çº§æ–¹å‘æ€§é…å¯¹æ£€æµ‹
        if cfg.target_stems:
            # ä¸­æ–‡æ³¨é‡Šï¼šTargetæ¨¡å¼ä¸‹æŒ‰â€œç›®æ ‡é›†åˆ vs å‚è€ƒé›†åˆâ€åˆ†ç»„å‘é‡åŒ–ä¸ç´¢å¼•ï¼Œä»…åœ¨å‚è€ƒç´¢å¼•ä¸Šæœç´¢ï¼Œé¿å…å…¨æ’åˆ—ã€‚
            idx_a = [i for i, r in enumerate(rows) if r.sid in set(cfg.target_stems)]
            idx_b = [i for i, r in enumerate(rows) if r.sid in set(cfg.reference_stems or [])]
            rows_a = [rows[i] for i in idx_a]
            rows_b = [rows[i] for i in idx_b]
            if cfg.enable_multilingual:
                model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
                emb_a = build_multilingual_embeddings([r.text for r in rows_a], model_name=model_name, device=cfg.device)
                emb_b = build_multilingual_embeddings([r.text for r in rows_b], model_name=model_name, device=cfg.device)
            elif cfg.use_parallel and (cfg.device is None or cfg.device == 'cpu'):
                emb_a = build_embeddings_parallel([r.text for r in rows_a], model_name=cfg.model_name, device='cpu', num_workers=cfg.num_workers)
                emb_b = build_embeddings_parallel([r.text for r in rows_b], model_name=cfg.model_name, device='cpu', num_workers=cfg.num_workers)
            else:
                emb_a = build_embeddings([r.text for r in rows_a], model_name=cfg.model_name, device=cfg.device)
                emb_b = build_embeddings([r.text for r in rows_b], model_name=cfg.model_name, device=cfg.device)
            index_b = build_index(emb_b)
            pair_hits = detect_pairs_crossset(
                rows_a,
                rows_b,
                emb_a,
                emb_b,
                index_b,
                k=None,
                threshold=cfg.similarity_threshold,
                index_map_a=idx_a,
                index_map_b=idx_b,
            )
        else:
            pair_hits = detect_pairs(
                rows,
                embeddings,
                index,
                k=cfg.index_top_k,
                threshold=cfg.similarity_threshold,
            )
        
        # 5) èšåˆç»Ÿè®¡ï¼ˆå¯é€‰å¼•ç”¨æƒ©ç½šï¼‰
        stats = aggregate_pairs(
            rows,
            pair_hits,
            use_citation_penalty=cfg.enable_citation_check,
        )
        
        # 6) æ„å»ºè¯¦ç»†å‘½ä¸­è®°å½•
        details = build_pair_details(
            rows,
            stats,
            pair_hits,
            max_hits=cfg.max_hits_per_pair,
        )
        
        return stats, details

    def run_with_paragraphs(self) -> Tuple[List[dict], List[dict], List[dict], List[dict]]:
        """
        åŒæ—¶æ‰§è¡Œå¥å­çº§ä¸æ®µè½çº§æ£€æµ‹ã€‚

        Returns:
            (sent_stats, sent_details, para_stats, para_details)
        """
        cfg = self.config
        
        # å¥å­çº§æ£€æµ‹ï¼ˆé‡ç”¨ run()ï¼‰
        sent_stats, sent_details = self.run()
        
        if not cfg.enable_paragraph_check:
            return sent_stats, sent_details, [], []
        
        # æ®µè½çº§æ£€æµ‹
        paras = load_paragraphs(cfg.submissions_dir)
        if not paras:
            return sent_stats, sent_details, [], []
        
        if cfg.enable_multilingual:
            model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            para_embeddings = build_multilingual_embeddings(
                [p.text for p in paras],
                model_name=model_name,
                device=cfg.device,
            )
        elif cfg.use_parallel and (cfg.device is None or cfg.device == 'cpu'):
            para_embeddings = build_embeddings_parallel(
                [p.text for p in paras],
                model_name=cfg.model_name,
                device='cpu',
                num_workers=cfg.num_workers,
            )
        else:
            para_embeddings = build_embeddings(
                [p.text for p in paras],
                model_name=cfg.model_name,
                device=cfg.device,
            )
        
        # æ®µè½ç´¢å¼•
        para_index = build_index(para_embeddings)
        
        if cfg.target_stems:
            # ä¸­æ–‡æ³¨é‡Šï¼šæ®µè½çº§åŒæ ·é‡‡ç”¨è·¨é›†åˆæ£€ç´¢é€»è¾‘ï¼Œä»¥é˜ˆå€¼è¿‡æ»¤å‘½ä¸­ã€‚
            idx_pa = [i for i, p in enumerate(paras) if p.sid in set(cfg.target_stems)]
            idx_pb = [i for i, p in enumerate(paras) if p.sid in set(cfg.reference_stems or [])]
            paras_a = [paras[i] for i in idx_pa]
            paras_b = [paras[i] for i in idx_pb]
            if cfg.enable_multilingual:
                model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
                emb_pa = build_multilingual_embeddings([p.text for p in paras_a], model_name=model_name, device=cfg.device)
                emb_pb = build_multilingual_embeddings([p.text for p in paras_b], model_name=model_name, device=cfg.device)
            elif cfg.use_parallel and (cfg.device is None or cfg.device == 'cpu'):
                emb_pa = build_embeddings_parallel([p.text for p in paras_a], model_name=cfg.model_name, device='cpu', num_workers=cfg.num_workers)
                emb_pb = build_embeddings_parallel([p.text for p in paras_b], model_name=cfg.model_name, device='cpu', num_workers=cfg.num_workers)
            else:
                emb_pa = build_embeddings([p.text for p in paras_a], model_name=cfg.model_name, device=cfg.device)
                emb_pb = build_embeddings([p.text for p in paras_b], model_name=cfg.model_name, device=cfg.device)
            para_index_b = build_index(emb_pb)
            para_pair_hits = detect_paragraph_pairs_crossset(
                paras_a,
                paras_b,
                emb_pa,
                emb_pb,
                para_index_b,
                k=None,
                threshold=cfg.para_threshold,
                index_map_a=idx_pa,
                index_map_b=idx_pb,
            )
        else:
            para_pair_hits = detect_paragraph_pairs(
                paras,
                para_embeddings,
                para_index,
                k=cfg.para_top_k,
                threshold=cfg.para_threshold,
            )
        
        para_stats = aggregate_paragraph_pairs(paras, para_pair_hits)
        
        # æ®µè½è¯¦æƒ…ï¼ˆç®€ç‰ˆï¼‰ï¼šä¿ç•™æ ¸å¿ƒå­—æ®µï¼Œä¾¿äºç•Œé¢/æŠ¥å‘Šå±•ç¤º
        para_details = []
        for summary in para_stats:
            pair = tuple(summary["pair"])
            hits_raw = para_pair_hits.get(pair, [])[:cfg.max_hits_per_pair]
            
            para_matches = []
            for idx_i, idx_j, sim in hits_raw:
                para_i = paras[idx_i]
                para_j = paras[idx_j]
                para_matches.append({
                    "sid_i": para_i.sid,
                    "sid_j": para_j.sid,
                    "para_id_i": para_i.para_id,
                    "para_id_j": para_j.para_id,
                    "sim": float(sim),
                    "text_i": para_i.text[:200] + "..." if len(para_i.text) > 200 else para_i.text,
                    "text_j": para_j.text[:200] + "..." if len(para_j.text) > 200 else para_j.text,
                })
            
            para_details.append({
                "pair": list(pair),
                "score": summary["score"],
                "count": summary["count"],
                "mean_sim": summary["mean_sim"],
                "max_sim": summary["max_sim"],
                "coverage_min": summary["coverage_min"],
                "coverage_a": summary["coverage_a"],
                "coverage_b": summary["coverage_b"],
                "matches": para_matches,
            })
        
        return sent_stats, sent_details, para_stats, para_details

    def write_reports(
        self,
        stats: List[dict],
        details: List[dict],
        para_stats: List[dict] = None,
        para_details: List[dict] = None,
    ) -> None:
        """
        å†™å…¥å„ç§æŠ¥å‘Šæ–‡ä»¶ï¼ˆCSV/JSON/Wordï¼‰ã€‚

        Args:
            stats: å¥å­çº§ç»Ÿè®¡åˆ—è¡¨ã€‚
            details: å¥å­çº§è¯¦ç»†å‘½ä¸­åˆ—è¡¨ã€‚
            para_stats: æ®µè½çº§ç»Ÿè®¡åˆ—è¡¨ã€‚
            para_details: æ®µè½çº§è¯¦ç»†å‘½ä¸­åˆ—è¡¨ã€‚
        """
        output_dir = self.config.output_dir
        output_dir.mkdir(parents=True, exist_ok=True)

        # å¥å­çº§æŠ¥å‘Š
        write_summary_csv(output_dir / "pair_summary.csv", stats)
        write_pair_results(output_dir / "pair_results.json", details)
        write_evidence_top(output_dir / "evidence_top.json", details)
        
        # ç”ŸæˆWordæŠ¥å‘Š
        try:
            # è¯¦ç»†WordæŠ¥å‘Šï¼ˆåŒ…å«å…·ä½“åŒ¹é…å†…å®¹ï¼‰
            write_word_report(output_dir / "plagiarism_report.docx", stats, details)
            
            # æ±‡æ€»WordæŠ¥å‘Šï¼ˆä»…ç»Ÿè®¡ä¿¡æ¯ï¼‰
            write_word_summary_report(output_dir / "plagiarism_summary_report.docx", stats)
        except Exception as e:
            print(f"è­¦å‘Šï¼šç”ŸæˆWordæŠ¥å‘Šæ—¶å‡ºé”™ - {e}")
        
        # æ®µè½çº§æŠ¥å‘Š
        if para_stats and para_details:
            write_paragraph_summary(
                output_dir / "paragraph_summary.csv",
                para_stats
            )
            write_pair_results(
                output_dir / "paragraph_results.json",
                para_details
            )
            
            # æ®µè½çº§WordæŠ¥å‘Š
            try:
                # ä¸ºæ®µè½æ•°æ®æ·»åŠ hitså­—æ®µä»¥å…¼å®¹WordæŠ¥å‘Šæ ¼å¼
                para_details_with_hits = []
                for detail in para_details:
                    detail_copy = dict(detail)
                    detail_copy['hits'] = detail.get('matches', [])
                    para_details_with_hits.append(detail_copy)
                write_word_report(output_dir / "plagiarism_paragraph_report.docx", para_stats, para_details_with_hits)
            except Exception as e:
                print(f"è­¦å‘Šï¼šç”Ÿæˆæ®µè½çº§WordæŠ¥å‘Šæ—¶å‡ºé”™ - {e}")
    def run_with_agent(self) -> Tuple[List, List, List]:
        """
        å¸¦ Agent çš„å¥å­çº§æ£€æµ‹ä¸æ·±åº¦åˆ†ææµç¨‹ã€‚

        Returns:
            (sent_stats, sent_details, agent_reports)
        """
        print("\nğŸ” Step 1: Running standard plagiarism detection...")
        # æ‰§è¡Œå¸¸è§„æ£€æµ‹
        sent_stats, sent_details = self.run()
        print(f"âœ… Found {len(sent_stats)} document pairs with matches")
        
        # å¦‚æœæœªå¯ç”¨Agent
        if not self.config.enable_agent:
            print("âš ï¸ Agent is disabled in config")
            return sent_stats, sent_details, []
        
        print("\nğŸ¤– Step 2: Initializing AI Agent...")
        # å°è¯•å¯¼å…¥agentæ¨¡å—
        try:
            from .agent import SmartPlagiarismAgent, generate_agent_report, generate_agent_report_batch
            print("âœ… Agent module imported successfully")
        except ImportError as e:
            print(f"âŒ Failed to import agent module: {e}")
            return sent_stats, sent_details, []
        
        # åˆå§‹åŒ– Agent
        try:
            agent = SmartPlagiarismAgent(self.config.api_config_path, dual_phase=self.config.agent_dual_phase)
            print(f"âœ… Agent initialized with config: {self.config.api_config_path}")
        except Exception as e:
            import traceback
            print(f"âŒ Agent initialization failed:")
            print(f"   Error: {e}")
            traceback.print_exc()
            return sent_stats, sent_details, []
        
        print(f"\nğŸ“Š Step 3: Filtering candidates (threshold >= {self.config.agent_threshold})...")
        agent_reports = []
        candidates = [d for d in sent_details if d.get('score', 0) >= self.config.agent_threshold]
        candidates.sort(key=lambda x: x.get('score', 0), reverse=True)
        
        print(f"   Total pairs: {len(sent_details)}")
        print(f"   Candidates above threshold: {len(candidates)}")
        if candidates:
            print(f"   Top candidate score: {candidates[0].get('score', 0):.3f}")
        else:
            print(f"   âš ï¸ No pairs meet the threshold of {self.config.agent_threshold}")
            return sent_stats, sent_details, []
        
        # å¤„ç†ç¼“å­˜
        output_dir = self.config.output_dir
        cache_path = output_dir / "agent_cache.json"
        cache = {}
        if cache_path.exists():
            try:
                cache = json.loads(cache_path.read_text(encoding='utf-8'))
                print(f"   ğŸ“¦ Loaded {len(cache)} cached reports")
            except Exception as e:
                print(f"   âš ï¸ Failed to load cache: {e}")
                cache = {}
        
        # åˆ†ç¦»å·²ç¼“å­˜å’Œæ–°çš„å€™é€‰
        new_candidates = []
        for d in candidates:
            key = f"{d['pair'][0]}__{d['pair'][1]}"
            if key in cache:
                agent_reports.append({'pair': d['pair'], 'report': cache[key]})
                print(f"   â™»ï¸ Using cached report for {d['pair'][0]} vs {d['pair'][1]}")
            else:
                new_candidates.append(d)
        
        print(f"   Cached reports: {len(agent_reports)}")
        print(f"   New candidates to analyze: {len(new_candidates)}")
        
        # ç¡®å®šéœ€è¦ç”Ÿæˆçš„æŠ¥å‘Šæ•°é‡
        max_reports = self.config.agent_max_reports if self.config.agent_max_reports > 0 else len(new_candidates)
        limit = min(max_reports - len(agent_reports), len(new_candidates))
        batch = new_candidates[:limit]
        
        print(f"   Will generate {len(batch)} new reports (max: {max_reports})")
        
        if batch:
            print(f"\nğŸ”¬ Step 4: Generating AI analysis reports...")
            # è¯»å–æ–‡æœ¬
            texts = {}
            for d in batch:
                a, b = d['pair'][0], d['pair'][1]
                if a not in texts:
                    texts[a] = self._read_full_text(a)
                    print(f"   ğŸ“„ Loaded text for {a} ({len(texts[a])} chars)")
                if b not in texts:
                    texts[b] = self._read_full_text(b)
                    print(f"   ğŸ“„ Loaded text for {b} ({len(texts[b])} chars)")
            
            # æ‰¹é‡ç”ŸæˆæŠ¥å‘Š
            print(f"\n   ğŸš€ Calling AI API for batch analysis...")
            success_count = 0
            try:
                batched = generate_agent_report_batch(agent, batch, texts, dual_phase=self.config.agent_dual_phase)
                for item in batched:
                    agent_reports.append(item)
                    key = f"{item['pair'][0]}__{item['pair'][1]}"
                    cache[key] = item['report']
                    success_count += 1
                    print(f"   âœ… Report {success_count}/{len(batch)}: {item['pair'][0]} vs {item['pair'][1]}")
            except Exception as batch_error:
                print(f"   âš ï¸ Batch processing failed: {batch_error}")
                print(f"   ğŸ”„ Falling back to individual report generation...")
                
                # é€ä¸ªç”ŸæˆæŠ¥å‘Š
                for idx, detail in enumerate(batch, 1):
                    try:
                        print(f"   ğŸ”„ Generating report {idx}/{len(batch)}...")
                        a = self._read_full_text(detail['pair'][0])
                        b = self._read_full_text(detail['pair'][1])
                        report = generate_agent_report(agent, detail, a, b, dual_phase=self.config.agent_dual_phase)
                        agent_reports.append({'pair': detail['pair'], 'report': report})
                        key = f"{detail['pair'][0]}__{detail['pair'][1]}"
                        cache[key] = report
                        success_count += 1
                        print(f"   âœ… Success: {detail['pair'][0]} vs {detail['pair'][1]}")
                    except Exception as e:
                        print(f"   âŒ Failed: {detail['pair'][0]} vs {detail['pair'][1]}")
                        print(f"      Error: {e}")
                        continue
            
            print(f"\n   ğŸ“ˆ Successfully generated {success_count}/{len(batch)} reports")
            
            # ä¿å­˜ç¼“å­˜
            try:
                cache_path.write_text(json.dumps(cache, ensure_ascii=False, indent=2), encoding='utf-8')
                print(f"   ğŸ’¾ Cache saved to {cache_path}")
            except Exception as e:
                print(f"   âš ï¸ Failed to save cache: {e}")
        
        print(f"\nâœ… Agent analysis complete: {len(agent_reports)} total reports available\n")
        return sent_stats, sent_details, agent_reports

    def _read_full_text(self, student_id: str) -> str:
        """è¯»å–æŒ‡å®šå­¦ç”Ÿçš„å®Œæ•´æ–‡æœ¬å†…å®¹ã€‚"""
        from .corpus import iter_documents
        for sid, doc_path in iter_documents(self.config.submissions_dir):
            if sid == student_id:
                return doc_path.read_text(encoding='utf-8', errors='ignore')
        return ""
